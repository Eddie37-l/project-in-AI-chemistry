import argparse
import math
import os
import torch

from datasets import load_from_disk
from transformers import (
    RobertaTokenizerFast,
    RobertaForMaskedLM,
    Trainer,
    TrainingArguments,
)

# ----------- arguments CLI --------------------------------------
parser = argparse.ArgumentParser()
parser.add_argument("--dataset_dir", required=True,
                    help="Dossier qui contient le sous-dossier test/ sauvegard√© par datasets.save_to_disk()")
parser.add_argument("--model_dir",   required=True,
                    help="Dossier du mod√®le pr√©-entra√Æn√© (config, tokenizer, poids)")
parser.add_argument("--batch_size",  type=int, default=8,
                    help="Taille de batch pour l‚Äô√©valuation")
parser.add_argument("--output_dir",  default="output/test_eval",
                    help="O√π √©crire les logs locaux (pas le mod√®le)")
parser.add_argument("--wandb_project", default=None,
                    help="Nom de projet W&B si tu veux logger (laisser vide pour d√©sactiver)")
args = parser.parse_args()


# ----------- chargement mod√®le + tokenizer ----------------------
print("üîπ Loading model & tokenizer‚Ä¶")
tokenizer = RobertaTokenizerFast.from_pretrained(args.model_dir)
model     = RobertaForMaskedLM.from_pretrained(args.model_dir)

device = torch.device(
    "cuda" if torch.cuda.is_available()
    else "mps" if torch.backends.mps.is_available()
    else "cpu"
)
model.to(device)
print("‚úÖ Model on", device)

# ----------- chargement dataset test ----------------------------
test_path = os.path.join(args.dataset_dir, "test")
print(f"üîπ Loading test set from {test_path} ‚Ä¶")
ds_test = load_from_disk(test_path)
print(f"‚úÖ Test set size : {len(ds_test)}")

# ----------- ajout labels s‚Äôils n‚Äôexistent pas ------------------
if "labels" not in ds_test.column_names:
    print("üõ†  Adding 'labels' column for loss computation ‚Ä¶")
    ds_test = ds_test.map(lambda ex: {"labels": ex["input_ids"]})

# ----------- TrainingArguments (√©val uniquement) ----------------
eval_args = TrainingArguments(
    output_dir=args.output_dir,
    per_device_eval_batch_size=args.batch_size,
    do_train=False,
    do_eval=True,
)

# ----------- Trainer et √©valuation ------------------------------
trainer = Trainer(model=model, args=eval_args, tokenizer=tokenizer)

print("üöÄ Evaluating on test set ‚Ä¶")
metrics = trainer.evaluate(eval_dataset=ds_test)
# metrics contient normalement 'eval_loss'

eval_loss = metrics.get("eval_loss")
if eval_loss is None:
    print("‚ö†Ô∏è  eval_loss manquante ; v√©rifie que 'labels' est bien pr√©sent.")
else:
    perplexity = math.exp(eval_loss) if eval_loss < 100 else float("inf")
    metrics["perplexity"] = perplexity
    print(f"üìâ  Eval loss      : {eval_loss:.4f}")
    print(f"üß†  Perplexity     : {perplexity:.2f}")

print("‚úÖ Done.")
